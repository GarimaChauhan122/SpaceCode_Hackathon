{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6068a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f2f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fd1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c98602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ce88c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Execution\n"
     ]
    }
   ],
   "source": [
    "from htru1 import HTRU1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0a2ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # randomly flip \n",
    "    transforms.RandomRotation(10), # randomly rotate\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f12bcbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "49005\n",
      "995\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "trainset = HTRU1(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# Print the unique class labels\n",
    "\n",
    "# Identify the majority and minority classes in the training set\n",
    "# You should replace these labels with the actual class labels used in your dataset\n",
    "majority_class = 1\n",
    "minority_class = 0\n",
    "\n",
    "# Find indices of majority and minority class samples in the training set\n",
    "train_majority_indices = [i for i, label in enumerate(trainset.targets) if label == majority_class]\n",
    "train_minority_indices = [i for i, label in enumerate(trainset.targets) if label == minority_class]\n",
    "\n",
    "print(len(train_majority_indices))\n",
    "print(len(train_minority_indices))\n",
    "\n",
    "# Randomly under-sample majority class in the training set to balance the classes\n",
    "num_samples_to_keep_train = (min(len(train_majority_indices), len(train_minority_indices)))\n",
    "print(num_samples_to_keep_train)\n",
    "selected_train_majority_indices = np.random.choice(train_majority_indices, size=int(1.0765*num_samples_to_keep_train), replace=False)\n",
    "\n",
    "# Combine indices of both classes to create a balanced training set\n",
    "balanced_train_indices = np.concatenate([selected_train_majority_indices, train_minority_indices])\n",
    "\n",
    "# Use the balanced indices to create the balanced training set\n",
    "balanced_trainset = torch.utils.data.Subset(trainset, balanced_train_indices)\n",
    "\n",
    "# Create a DataLoader for the balanced training set\n",
    "trainloader = torch.utils.data.DataLoader(balanced_trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "#trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6cef254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d9705309",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6d5b0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e2b34053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.062\n",
      "[1,    20] loss: 0.026\n",
      "[1,    30] loss: 0.150\n",
      "[1,    40] loss: 0.034\n",
      "[1,    50] loss: 0.102\n",
      "[1,    60] loss: 0.121\n",
      "[1,    70] loss: 0.055\n",
      "[1,    80] loss: 0.371\n",
      "[1,    90] loss: 0.073\n",
      "[1,   100] loss: 0.042\n",
      "[1,   110] loss: 0.024\n",
      "[1,   120] loss: 0.064\n",
      "[1,   130] loss: 0.105\n",
      "[1,   140] loss: 0.033\n",
      "[1,   150] loss: 0.096\n",
      "[1,   160] loss: 0.115\n",
      "[1,   170] loss: 0.043\n",
      "[1,   180] loss: 0.259\n",
      "[1,   190] loss: 0.052\n",
      "[1,   200] loss: 0.036\n",
      "[1,   210] loss: 0.078\n",
      "[1,   220] loss: 0.069\n",
      "[1,   230] loss: 0.515\n",
      "[1,   240] loss: 0.186\n",
      "[1,   250] loss: 0.124\n",
      "[1,   260] loss: 0.093\n",
      "[1,   270] loss: 0.091\n",
      "[1,   280] loss: 0.086\n",
      "[1,   290] loss: 0.076\n",
      "[1,   300] loss: 0.080\n",
      "[1,   310] loss: 0.116\n",
      "[1,   320] loss: 0.178\n",
      "[1,   330] loss: 0.146\n",
      "[1,   340] loss: 0.088\n",
      "[1,   350] loss: 0.051\n",
      "[1,   360] loss: 0.025\n",
      "[1,   370] loss: 0.241\n",
      "[1,   380] loss: 0.117\n",
      "[1,   390] loss: 0.087\n",
      "[1,   400] loss: 0.073\n",
      "[1,   410] loss: 0.021\n",
      "[1,   420] loss: 0.026\n",
      "[1,   430] loss: 0.294\n",
      "[1,   440] loss: 0.042\n",
      "[1,   450] loss: 0.063\n",
      "[1,   460] loss: 0.149\n",
      "[1,   470] loss: 0.126\n",
      "[1,   480] loss: 0.243\n",
      "[1,   490] loss: 0.118\n",
      "[1,   500] loss: 0.088\n",
      "[1,   510] loss: 0.105\n",
      "[2,    10] loss: 0.075\n",
      "[2,    20] loss: 0.056\n",
      "[2,    30] loss: 0.034\n",
      "[2,    40] loss: 0.183\n",
      "[2,    50] loss: 0.045\n",
      "[2,    60] loss: 0.019\n",
      "[2,    70] loss: 0.044\n",
      "[2,    80] loss: 0.034\n",
      "[2,    90] loss: 0.016\n",
      "[2,   100] loss: 0.134\n",
      "[2,   110] loss: 0.012\n",
      "[2,   120] loss: 0.103\n",
      "[2,   130] loss: 0.024\n",
      "[2,   140] loss: 0.195\n",
      "[2,   150] loss: 0.232\n",
      "[2,   160] loss: 0.175\n",
      "[2,   170] loss: 0.218\n",
      "[2,   180] loss: 0.076\n",
      "[2,   190] loss: 0.029\n",
      "[2,   200] loss: 0.121\n",
      "[2,   210] loss: 0.071\n",
      "[2,   220] loss: 0.093\n",
      "[2,   230] loss: 0.115\n",
      "[2,   240] loss: 0.123\n",
      "[2,   250] loss: 0.138\n",
      "[2,   260] loss: 0.190\n",
      "[2,   270] loss: 0.056\n",
      "[2,   280] loss: 0.171\n",
      "[2,   290] loss: 0.154\n",
      "[2,   300] loss: 0.029\n",
      "[2,   310] loss: 0.080\n",
      "[2,   320] loss: 0.147\n",
      "[2,   330] loss: 0.070\n",
      "[2,   340] loss: 0.468\n",
      "[2,   350] loss: 0.147\n",
      "[2,   360] loss: 0.205\n",
      "[2,   370] loss: 0.131\n",
      "[2,   380] loss: 0.058\n",
      "[2,   390] loss: 0.066\n",
      "[2,   400] loss: 0.026\n",
      "[2,   410] loss: 0.188\n",
      "[2,   420] loss: 0.090\n",
      "[2,   430] loss: 0.061\n",
      "[2,   440] loss: 0.062\n",
      "[2,   450] loss: 0.030\n",
      "[2,   460] loss: 0.158\n",
      "[2,   470] loss: 0.247\n",
      "[2,   480] loss: 0.014\n",
      "[2,   490] loss: 0.060\n",
      "[2,   500] loss: 0.133\n",
      "[2,   510] loss: 0.193\n",
      "[3,    10] loss: 0.062\n",
      "[3,    20] loss: 0.101\n",
      "[3,    30] loss: 0.103\n",
      "[3,    40] loss: 0.056\n",
      "[3,    50] loss: 0.078\n",
      "[3,    60] loss: 0.108\n",
      "[3,    70] loss: 0.048\n",
      "[3,    80] loss: 0.227\n",
      "[3,    90] loss: 0.132\n",
      "[3,   100] loss: 0.068\n",
      "[3,   110] loss: 0.010\n",
      "[3,   120] loss: 0.056\n",
      "[3,   130] loss: 0.090\n",
      "[3,   140] loss: 0.122\n",
      "[3,   150] loss: 0.022\n",
      "[3,   160] loss: 0.017\n",
      "[3,   170] loss: 0.248\n",
      "[3,   180] loss: 0.028\n",
      "[3,   190] loss: 0.076\n",
      "[3,   200] loss: 0.096\n",
      "[3,   210] loss: 0.065\n",
      "[3,   220] loss: 0.104\n",
      "[3,   230] loss: 0.057\n",
      "[3,   240] loss: 0.280\n",
      "[3,   250] loss: 0.052\n",
      "[3,   260] loss: 0.159\n",
      "[3,   270] loss: 0.208\n",
      "[3,   280] loss: 0.058\n",
      "[3,   290] loss: 0.081\n",
      "[3,   300] loss: 0.078\n",
      "[3,   310] loss: 0.152\n",
      "[3,   320] loss: 0.224\n",
      "[3,   330] loss: 0.086\n",
      "[3,   340] loss: 0.073\n",
      "[3,   350] loss: 0.054\n",
      "[3,   360] loss: 0.028\n",
      "[3,   370] loss: 0.469\n",
      "[3,   380] loss: 0.074\n",
      "[3,   390] loss: 0.171\n",
      "[3,   400] loss: 0.046\n",
      "[3,   410] loss: 0.137\n",
      "[3,   420] loss: 0.111\n",
      "[3,   430] loss: 0.053\n",
      "[3,   440] loss: 0.036\n",
      "[3,   450] loss: 0.171\n",
      "[3,   460] loss: 0.170\n",
      "[3,   470] loss: 0.100\n",
      "[3,   480] loss: 0.053\n",
      "[3,   490] loss: 0.116\n",
      "[3,   500] loss: 0.064\n",
      "[3,   510] loss: 0.070\n",
      "[4,    10] loss: 0.123\n",
      "[4,    20] loss: 0.008\n",
      "[4,    30] loss: 0.103\n",
      "[4,    40] loss: 0.068\n",
      "[4,    50] loss: 0.048\n",
      "[4,    60] loss: 0.243\n",
      "[4,    70] loss: 0.095\n",
      "[4,    80] loss: 0.100\n",
      "[4,    90] loss: 0.075\n",
      "[4,   100] loss: 0.172\n",
      "[4,   110] loss: 0.055\n",
      "[4,   120] loss: 0.102\n",
      "[4,   130] loss: 0.084\n",
      "[4,   140] loss: 0.129\n",
      "[4,   150] loss: 0.105\n",
      "[4,   160] loss: 0.062\n",
      "[4,   170] loss: 0.037\n",
      "[4,   180] loss: 0.074\n",
      "[4,   190] loss: 0.040\n",
      "[4,   200] loss: 0.057\n",
      "[4,   210] loss: 0.265\n",
      "[4,   220] loss: 0.086\n",
      "[4,   230] loss: 0.117\n",
      "[4,   240] loss: 0.048\n",
      "[4,   250] loss: 0.008\n",
      "[4,   260] loss: 0.062\n",
      "[4,   270] loss: 0.041\n",
      "[4,   280] loss: 0.067\n",
      "[4,   290] loss: 0.026\n",
      "[4,   300] loss: 0.049\n",
      "[4,   310] loss: 0.014\n",
      "[4,   320] loss: 0.007\n",
      "[4,   330] loss: 0.491\n",
      "[4,   340] loss: 0.014\n",
      "[4,   350] loss: 0.184\n",
      "[4,   360] loss: 0.065\n",
      "[4,   370] loss: 0.142\n",
      "[4,   380] loss: 0.099\n",
      "[4,   390] loss: 0.046\n",
      "[4,   400] loss: 0.025\n",
      "[4,   410] loss: 0.011\n",
      "[4,   420] loss: 0.114\n",
      "[4,   430] loss: 0.174\n",
      "[4,   440] loss: 0.140\n",
      "[4,   450] loss: 0.224\n",
      "[4,   460] loss: 0.248\n",
      "[4,   470] loss: 0.072\n",
      "[4,   480] loss: 0.090\n",
      "[4,   490] loss: 0.063\n",
      "[4,   500] loss: 0.199\n",
      "[4,   510] loss: 0.068\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nepoch = 4  # number of epochs\n",
    "\n",
    "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #labels = labels.float().unsqueeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "416114f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = HTRU1(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3a0d3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1a5ca95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3ae98779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of pulsar : 93 %\n",
      "Accuracy of nonpulsar : 97 %\n"
     ]
    }
   ],
   "source": [
    "classes = ('pulsar', 'nonpulsar')\n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "51dfef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9717\n"
     ]
    }
   ],
   "source": [
    "print((class_correct[0]+class_correct[1])/(class_total[0]+class_total[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
